#!/usr/bin/env python3

import os
import sys
import constants

from langchain.document_loaders import PyPDFLoader
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.llms import OpenAI

os.environ["OPENAI_API_KEY"] = constants.APIKEY

db_path = "~/Accumulator/Chroma"
os.makedirs(os.path.expanduser(db_path), exist_ok=True)

pdf_path = sys.argv[1]
query = sys.argv[2]

loader = PyPDFLoader(pdf_path)
pages = loader.load_and_split()

embeddings = OpenAIEmbeddings()
vectordb = Chroma.from_documents(pages, embeddings, persist_directory=db_path)
vectordb.persist()
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
pdf_qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.8), vectordb.as_retriever(), memory=memory)

result = pdf_qa({"question": query})
print("Answer:")
print(result["answer"])
